ssh kewu@nome.nmc-probe.org

(perform on master node)
rsync -avz /proj/ClusterSched16/719 /groups/ClusterSched16/advcc10/

ssh kewu@m.[experiment-name].ClusterSched16.nome.nx

cd /groups/ClusterSched16/advcc10/719/bin

export PATH="/groups/ClusterSched16/advcc10/719/bin:$PATH"

generate a hostfile with all hosts that are part of your experiment
go to the emulab login page -> Experimentation->Experiment List -> YOUREXPERIMENT -> Details tab
copy the list of machines to ~/tetris.deth  on your " master node "

m.Phase3Team1010.ClusterSched16.nome.nx
s-1.Phase3Team1010.ClusterSched16.nome.nx
s-2.Phase3Team1010.ClusterSched16.nome.nx
s-3.Phase3Team1010.ClusterSched16.nome.nx
s-4.Phase3Team1010.ClusterSched16.nome.nx

cd ~
tetris.sh

rsync /proj/ClusterSched16/images/debian-current.qcow.GOLD /groups/ClusterSched16/advcc10/debian-current.qcow

check to see if  /groups/ClusterSched16/advcc10/719/bin/gennetmaster.py has a line with:
rackcap = [1,6,6,6,6]

On the "master node", as root
cd /groups/ClusterSched16/advcc10/719/bin
sudo ./net-master.sh 10

On each slave node, as root:
cd /groups/ClusterSched16/advcc10/719/bin
sudo ./net-slave.sh

On the "master node", as root
cd /groups/ClusterSched16/advcc10/719/bin
sudo ./quickstart.sh 10


on a new shell
ssh kewu@m.[experiment-name].ClusterSched16.nome.nx
ssh root@10.10.1.10
password: changeme123
passwd

addgroup --gid 6043 --force-badname Clu-advcc101
adduser --uid 10160 --gid 6043 kewu
adduser --uid 10166 --gid 6043 linquanc

mkdir ~/.ssh/
vim ~/.ssh/authorized_keys
vim ~/.ssh/config
Host *
    ForwardAgent yes

vim ~/.ssh/id_dsa
vim ~/.ssh/id_dsa.pub

chmod 600 ~/.ssh/authorized_keys
chmod 600 ~/.ssh/config
chmod 600 ~/.ssh/id_dsa
chmod 600 ~/.ssh/id_dsa.pub

Add your new user to the /etc/sudoers file
chmod 660 /etc/sudoers
kewu        ALL=NOPASSWD: ALL
linquanc    ALL=NOPASSWD: ALL
chmod 440 /etc/sudoers


vim /etc/fstab
10.10.1.1

/sbin/shutdown -h now

update your copy of the scripts mentioned below to point to the location of your  VM image./
sudo ./rackstart.sh r0 8G 8

Subsequent VMs are started on a rack by rack basis. Login to each of the remaining four physical nodes (s-1 to s-4) and start up a batch of VMs on that physical node as follows:
sudo ./rackstart.sh r1 4G 2
sudo ./rackstart.sh r2 4G 2
sudo ./rackstart.sh r3 4G 2
sudo ./rackstart.sh r4 4G 2


====  for phase2    ====
sudo ./rackstart.sh r0 16G 8 
sudo ./rackstart.sh r1 8G 4
sudo ./rackstart.sh r2 4G 2
sudo ./rackstart.sh r3 4G 2 
sudo ./rackstart.sh r4 4G 2 
==== end for phase2 ====

ssh root@10.10.1.10
ssh root@10.10.[2-5].[10-15]

login to master host as kewu (NOT as root). then run:
cd /groups/ClusterSched16/advcc10/719/bin
./mpssh -s -f vmips date


As root of master host:
apt-get install nfs-kernel-server
mkdir -p /opt/projects/advcc
echo "/opt/projects/advcc 10.10.*.*(rw,sync,no_root_squash)" >> /etc/exports
service nfs-kernel-server start

The VM should already have the /etc/fstab entry to mount this share. Please make sure youâ€™ve changed all occurrences of 10.10.1.10 in /etc/fstab to 10.NETID.1.10 during your VM configuration step and persist it.

sudo su
cp /proj/ClusterSched16/hadoop-2.2-advcc.tar.gz /opt/projects/advcc/hadoop-2.2-advcc.tar.gz
cd /opt/projects/advcc
tar -xzvf hadoop-2.2-advcc.tar.gz

Change /opt/projects/advcc/hadoop/scripts/setup-scratch-user.sh to use your andrew ID and group ID

Install Java 6

in master node 
vim ~/.bashrc
export HADOOP_HOME=/opt/projects/advcc/hadoop/hadoop-2.2.0
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HADOOP_PREFIX=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64


Modify yarn.tetris.hostnames in yarn-site.xml* under $HADOOP_CONF_DIR r1h0,r1h1,r1h2,r1h3,r1h4,r1h5,r2h0,r2h1,r2h2,r2h3,r2h4,r2h5,r3h0,r3h1,r3h2,r3h3,r3h4,r3h5,r4h0,r4h1,r4h2,r4h3,r4h4,r4h5
amnode/yarn-site.xml
gpurack/yarn-site.xml
yarn-site.xml
yarn-site.xml.CapacityScheduler
yarn-site.xml.TetriScheduler

===  for phase 2  ===
r1h0,r1h1,r1h2,r1h3,r2h0,r2h1,r2h2,r2h3,r2h4,r2h5,r3h0,r3h1,r3h2,r3h3,r3h4,r3h5,r4h0,r4h1,r4h2,r4h3,r4h4,r4h5
=== end for pahse 2 ===

exit from VM, run on master node
Untar /proj/ClusterSched16/exp-advcc.tgz  to $HADOOP_HOME

ssh to r0h0 VM
./replayTrace.py -c config-mini -t traceMPI-mini

from r0h0, as user, ssh to all slave VMs (you can script it) and distribute the resulting r0h0:~/.ssh/known_hosts to all slave VMs. This is needed for mpirun to be able to start MPI jobs from any VM.

copying $HADOOP_CONF_DIR/yarn-site.xml.TetriScheduler to $HADOOP_CONF_DIR/yarn-site.xml (or creating a symlink)

When the scheduling policy is changed, you need to restart the resource manager.
$HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager



$HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode
$HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager
$HADOOP_YARN_HOME/sbin/yarn-daemon.sh stop proxyserver --config $HADOOP_CONF_DIR
$HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh stop historyserver --config $HADOOP_CONF_DIR
$HADOOP_HOME/sbin/yarn-daemon.sh  --config $HADOOP_CONF_DIR/amnode stop nodemanager